{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4 - Deep Reinforcement Learning"
      ],
      "metadata": {
        "id": "NcDhlfqyBd6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1"
      ],
      "metadata": {
        "id": "-id00ye6CNLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**Episodic Tasks:** In episodic tasks, the agent's interaction with the environment is segmented into distinct episodes. Each episode is a self-contained unit with a clear beginning and end. The termination of an episode occurs when the agent reaches a terminal state, which is a specific condition defined by the task. One of the main characteristics of episodic tasks is that the end of one episode resets the environment, effectively severing any direct influence of the previous episode's outcome on the next one. This reset mechanism allows the agent to start each episode afresh, often from a randomized or fixed initial state. An example of an episodic task is the game of Pac-Man. Each game (episode) starts with Pac-Man in a fixed starting position. The game ends (reaches a terminal state) when Pac-Man loses all lives. The next game starts without any memory of the previous game’s performance. This episodic nature allows for focused learning on short-term strategies within each game.\n",
        "\n",
        "**Continous Tasks:** Continuous tasks are significantly that episodic tasks in that they lack distinct episodic boundaries. The agent's experience in these tasks is a seamless, ongoing interaction with the environment. These tasks are characterized by the absence of terminal states that reset the environment. A key aspect of continuous tasks is the requirement for the agent to develop and maintain long-term strategies. Since the environment does not reset, the consequences of the agent's actions persist and influence future decisions. An  example of a continous task involves managing a power grid. The system continuously adjusts to varying power demands and supply conditions. There is no terminal state where the system resets. Instead, the agent must continuously adapt its strategy to ensure a stable power supply over time."
      ],
      "metadata": {
        "id": "TbEBvSo3ClZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2"
      ],
      "metadata": {
        "id": "e1D_yfneCWqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**Exploration:** Exploration refers to the strategy where an RL agent chooses actions that are not known to yield the highest reward but might potentially discover more rewarding actions. It's about taking risks and trying out less familiar options. Exploration is crucial in the early stages of learning or in dynamic environments, where the agent lacks sufficient knowledge about the consequences of its actions. It allows the agent to gather diverse experiences and learn more about the environment.\n",
        "\n",
        "**Exploitation:** Exploitation involves choosing actions that the agent believes will yield the highest reward based on its current knowledge. It's about leveraging what the agent has already learned. As the agent learns more about the environment, exploitation becomes increasingly important to maximize the rewards, especially in stable or well-understood environments.\n",
        "\n",
        "**ε-Greedy:** The ε-greedy policy is a simple method to balance exploration and exploitation. Here, ε (epsilon) represents the probability of choosing an action at random (exploration), while 1-ε is the probability of choosing the best-known action (exploitation). This policy is employed because it provides a straightforward mechanism for the agent to occasionally deviate from the known best strategy and explore new actions. This is important for discovering potentially better strategies that the agent might not encounter if it only exploits.\n",
        "\n",
        "**Fixed vs. Variable ε:** Generally, keeping ε fixed or varying it over time (scheduled ε) depends on the specific requirements of the task and the environment. On one hand, a fixed ε might be suitable in environments where the dynamics do not change significantly over time. On the other, variable ε (scheduled ε) is more common where ε is varied over time. Typically, ε startes high (encouraging exploration) and gradually decreases (shifting towards exploitation). This approach aligns with the natural learning progression of the agent, where more exploration is beneficial initially, and more exploitation is desirable as the agent learns. With that said, it is important to ensure that there is a right balance between exploration and exploitation with ε. A higher value of ε means more exploration. This is beneficial early in training when the agent knows little about the environment and needs to gather a wide range of experiences. A lower value of ε leads to more exploitation. This is useful later in training when the agent has already learned about the environment and needs to maximize the reward based on this knowledge. The value of ε, therefore, helps in balancing exploration (trying new things for better understanding) and exploitation (using the best-known strategy for maximum reward)."
      ],
      "metadata": {
        "id": "3SDulg8cCnRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3"
      ],
      "metadata": {
        "id": "7eXCL_NyGy2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The difference between deep Q-learning and Q-Learning is that Q-Learning is a value-based reinforcement learning algorithm that learns the value of an action in a particular states. It uses a Q-table, a lookup table where we calculate the maximum expected future rewards for action at each state. This table guides the agent by telling it which action is best at each state. In Q-learning, the algorithm updates the Q-values (expected future rewards) of each action at each state. It does this by using the Bellman equation during the learning process. As the agent explores the environment, it fills out the Q-table, using it to make decisions based on the maximization of expected reward. The major limitation of Q-learning is its impracticality in dealing with high-dimensional state spaces or continuous action spaces. The Q-table grows exponentially with the number of states and actions, making it infeasible for complex environments.On the other hand, deep Q-Learning modifies Q-learning by using a deep neural network to approximate the Q-value function. Instead of a Q-table, the algorithm uses a neural network (called a Deep Q-Network or DQN) that takes in a state and outputs Q-values for each action. The neural network enables Deep Q-Learning to handle environments with high-dimensional state spaces, like images from video games, which would be impossible to represent with a Q-table. Deep Q-Learning utilizes a technique called experience replay, where the agent stores its experiences in a replay buffer and randomly samples from this buffer to train the neural network. This approach breaks the correlation between consecutive learning samples, therefore stabilizing the learning process. Another key feature of Deep Q-Learning is the use of a target network. This is a separate neural network which is a copy of the main network but with its weights frozen for a certain number of steps. This target network helps in stabilizing the training process by providing a stable target for the main network to train against.\n",
        "\n",
        "**Steps of the Deep Q-Learning Algorithm**\n",
        "\n",
        "1) Start by setting up a replay memory D with a maximum storage capacity N. This memory will store experiences of the agent, each of which comprises the state, action taken, reward received, and the next state.\n",
        "\n",
        "2) Create a neural network to represent the action-value function Q, which estimates the value of taking a certain action in a given state. Initialize this network with random weights, identifyig the start of the learning process.\n",
        "\n",
        "3) Start of an Episode → each episode is an instance of the agent interacting with the environment from a starting state to a terminal state.\n",
        "\n",
        "4) Initialize the sequence s1 with the initial state x1 and preprocess this sequence to ϕ1. Preprocessing typically involves converting raw sensory input (like pixels) into a more manageable form for the neural network.\n",
        "\n",
        "5) At each timestep, choose an action based on the ε-greedy policy. With probability ε, select a random action for exploration. Otherwise, choose the action with the highest estimated value according to the Q-network.\n",
        "\n",
        "6) Execute the chosen action in the environment, then observe the reward and the next state\n",
        "\n",
        "7) Store the transition (the current preprocessed state, actiuon, reward, and the next preprocessed state) in the replay memory D.\n",
        "\n",
        "8) Randomly sample a minibatch of transitions from the replay memory. This step is crucial for breaking correlations between consecutive learning samples, which helps in stabilizing the learning process.\n",
        "\n",
        "9) If the next state is terminal, the target is just the observed reward. Otherwise, it is the observed reward plus the discounted maximum Q value for the next state, as estimated by the Q-network.\n",
        "\n",
        "10) Update the Q-network by performing a gradient descent step to minimize the difference between the predicted Q values and the target values set in the previous step. This step gradually improves the Q-network's accuracy in estimating action values."
      ],
      "metadata": {
        "id": "tjo-oTDxCole"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4"
      ],
      "metadata": {
        "id": "_BuGX6EVG4uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "In Q-learning using neural networks, the Q-values are updated at every step. This can lead to rapid and significant changes in the policy, which the next updates depend on. Such frequent updates can destabilize the learning process because the network is essentially chasing a moving target. Therefore, the target Q-network, which is a copy of the main Q-network but with its weights frozen for a set number of steps, provides a stable target for the main network. By keeping the target Q-values fixed over several updates, the learning process becomes more stable. This stability is crucial for the convergence of the neural network.\n",
        "\n",
        "In deep learning-based Q-learning, there's a risk of feedback loops. If the Q-network's outputs are immediately used to update its own targets, the network might reinforce its own biases or errors. By using a separate, slowly updated target network, the immediate feedback loop is broken. This separation ensures that the learning is grounded in a less volatile, more reliable set of target values, reducing the risk of harmful feedback loops and enabling more robust learning. Without a target network, the estimated Q-values can fluctuate significantly, making it hard for the agent to discern which actions are genuinely better. This fluctuation can be especially pronounced in complex environments where many actions have similar values. In other words, the target Q-network provides a more consistent set of Q-values to compare against, smoothing out the learning process. This consistency helps the main network to better differentiate between the values of different actions, leading to better learning."
      ],
      "metadata": {
        "id": "RyQOE5AhHBxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5"
      ],
      "metadata": {
        "id": "7bKbdum_NYpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "Experience replay enhances the efficiency of Q-learning, particularly in its deep learning variant. This technique involves storing the agent's experiences and then randomly sampling from this collection to train the model. The issue with Q-learning is that it involves learning from consecutive experiences, which can lead to strong correlations between these experiences. This correlation can make the learning process less efficient and potentially lead to the overfitting of the model to recent experiences. Experience replay breaks these correlations by storing experiences (comprising states, actions, rewards, and subsequent states) and later sampling randomly from this pool. This random sampling ensures that the learning updates are based on a diverse set of experiences, leading to a more robust and generalized learning process. Additionally, each experience in Q-Learning is typically used only once for a learning update and then discarded. This approach can be inefficient, particularly in complex environments where valuable experiences might be rare. Experience replay allows for each stored experience to be used multiple times in different learning updates. This reuse of experiences makes the learning process more efficient by extracting more value from each experience. It also means that the agent can learn from both rare and common experiences over a longer period, leading to a more general and comprehensive understanding of the environment."
      ],
      "metadata": {
        "id": "N943kCNcNYpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6"
      ],
      "metadata": {
        "id": "u1LATb1uNY0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "Prioritized experience aims to make the learning process more efficient by strategically selecting experiences from the replay memory based on their potential for learning. In experience replay, experiences (consisting of states, actions, rewards, and subsequent states) are sampled randomly from a replay buffer. Prioritized experience replay modifies this by assigning a priority to each experience in the memory. Experiences that are deemed more valuable for learning are more likely to be sampled. This is important because not all experiences are equally valuable in terms of what the agent can learn from them. Some experiences may be rare or provide significant information about the environment, making them more valuable for learning.\n",
        "\n",
        "**Calculating Priority for Experiences:**\n",
        "- **TD Error (Temporal Difference Error)**: The priority of an experience is often determined by its Temporal Difference (TD) error. TD error is the difference between the predicted Q-value and the observed Q-value (the reward plus the discounted future reward) for a given state-action pair. Higher TD errors indicate that the prediction was less accurate, implying that the agent has more to learn from that experience. Sometimes, a factor of recency is also included, giving a slightly higher priority to newer experiences, assuming they may be more relevant to the current state of the environment. Additionally, experiences that have been sampled less frequently in the past may be given a higher priority to ensure a diverse range of experiences is used for learning.\n",
        "\n",
        "Overall, prioritized replay focuses the agent’s learning on experiences from which it can learn the most, leading to more efficient and faster learning. By not solely relying on TD error and incorporating factors like recency and sampling frequency, the approach avoids overfitting to experiences with high TD errors. This targeted approach allows for faster convergence and a more robust learning process, especially in complex environments where certain experiences are significantly more informative than others."
      ],
      "metadata": {
        "id": "WuvDJvDwNY0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7"
      ],
      "metadata": {
        "id": "SyFsFl-YNZFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**Similarities Between GORILA and Ape-X:**\n",
        "1. Both GORILA and Ape-X are designed for distributed reinforcement learning. They utilize multiple workers (agents) to interact with the environment in parallel, which accelerates the learning process and allows for scalability.\n",
        "2. Both architectures employ the concept of experience replay. This approach involves storing experiences (state, action, reward, next state) and then sampling from this pool to update the learning models. This helps in breaking the correlation between consecutive learning samples and stabilizes the learning process.\n",
        "3. In both GORILA and Ape-X, the experience collection is decentralized. Individual workers collect experiences independently, which are then aggregated in a central replay memory or distributed across multiple replay memories. This decentralization allows for diverse experience collection, enhancing the robustness of the learning process.\n",
        "\n",
        "**Differences Between GORILA and Ape-X:**\n",
        "1. In GORILA, there is a clear separation between the components responsible for experience generation (actors), learning (learners), and the central neural network model (parameter server). The actors send their experiences to the central replay memory and receive updated network parameters from the parameter server. Ape-X, on the other hand, emphasizes a more hierarchical architecture. It employs a large number of actors that send their experiences to a prioritized replay buffer. A smaller number of learner nodes sample from this buffer and independently update their neural network models.\n",
        "2. GORILA uses a standard experience replay mechanism where experiences are uniformly sampled from the replay buffer. Ape-X implements a prioritized experience replay system where experiences are sampled according to their priority (based on the TD error), which makes the learning process more efficient by focusing on more 'informative' experiences.\n",
        "3. In GORILA, there is a centralized model, and the learning updates from different learners are aggregated to update this central model. The actors periodically synchronize their local models with the central model. In contrast, Ape-X allows for more independence among learners. Each learner updates its own model without a central model aggregation step. This independence can lead to more diverse learning but may require mechanisms to ensure consistency among the learners' models."
      ],
      "metadata": {
        "id": "9Y4ioC-oNZFO"
      }
    }
  ]
}