{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1 - Staleness in Asynchronous SGD"
      ],
      "metadata": {
        "id": "NcDhlfqyBd6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "- **Staleness** â†’ refers to the number of weight updates that have occurred between the time a learner reads the weights and the time it updates the weights based on its gradient calculation\n",
        "- Learner 1 is faster, running at 2.5x the speed of Learner 2\n",
        "- The updates to the weights are instant once a gradient is received by the parameter server\n",
        "\n",
        "**Calculation for staleness in an Asynchronous SGD (Stochastic Gradient Descent) system** = counting the number of weight updates that occur between the time a learner reads the weights and the time it sends its gradient to the parameter server\n",
        "\n",
        "In this specific case, the staleness for each gradient is as follows:\n",
        "\n",
        "1. **g[L1, 1]** at second 1\n",
        "- No previous updates, staleness is 0.\n",
        "\n",
        "2. **g[L1, 2]** at second 2\n",
        "- Since the previous update from Learner 1 (g[L1, 1]) has occurred, staleness for Learner 1's second graident also carries a staleness value of 0 since Learer 2 has not computed a gradient yet.\n",
        "\n",
        "3. **g[L2, 1]** at second 2.5\n",
        "- Learner 2's first gradient has a staleness value of 0 as well since it is Learner 2's first gradient.\n",
        "\n",
        "4. **g[L1, 3]** at second 3\n",
        "- For Learner 1's third gradient, there is staleness of 1 since g[L2,1] has occured. In other words, Learner 2 has only computed one gradient since the last Learner 1 occurence. In other words, considering that the Learner 2 computation has occured after Learner 1's second gradient computation, we can conclude that the staleness value is 1.  \n",
        "\n",
        "5. **g[L1, 4]** at second 4\n",
        "- Learner 2 has not computed any gradients after g[L2,1] and before g[L1,4]. Since the last Learner 1's update (third gradient), Learner 2 has not computed any gradients for Learner 1 fourth's gradient. In other words, Learner 1's forth gradient therefore has a staleness value of 0.\n",
        "\n",
        "6. **g[L2, 2]** at second 5\n",
        "- Thus far, Learner 1 has computed a total of 2 gradeints ((g[L1,2] and g[L1,3]) and Learner 2 has computed one gradeint g[L2,1]. Therefore, the staleness of Learner 2's second gradient is 2.\n",
        "\n",
        "*To summarize, the staleness for each gradient is as follows:*\n",
        "\n",
        "- **g[L1, 1]: 0**\n",
        "- **g[L1, 2]: 0**\n",
        "- **g[L2, 1]: 0**\n",
        "- **g[L1, 3]: 1**\n",
        "- **g[L1, 4]: 0**\n",
        "- **g[L2, 2]: 2**"
      ],
      "metadata": {
        "id": "s1w0-Qn-CAZl"
      }
    }
  ]
}