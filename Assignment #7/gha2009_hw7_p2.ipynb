{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Data Parallelism in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs = 4\n"
     ]
    }
   ],
   "source": [
    "# Import torch library\n",
    "import torch\n",
    "\n",
    "# Set the device to 'cuda' to use the GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Print the total number of GPUs available\n",
    "print(\"Number of GPUs =\", torch.cuda.device_count()) # torch.cuda.device_count() returns the number of CUDA-enabled GPUs detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    ''' A Pre-activation version of the BasicBlock used in ResNet architectures. '''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        # First apply Batch Normalization and then a ReLU activation\n",
    "        # before each convolutional layer\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        # A shortcut to match the input and output dimensions, if needed\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the block\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut  # Adding the shortcut to the main path\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    ''' A Pre-activation version of the Bottleneck module, more complex than the basic block. '''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "\n",
    "        # Shortcut for dimension matching\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the bottleneck\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut  # Adding the shortcut to the main path\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    ''' The overall Pre-activation ResNet architecture. '''\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # The initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        # Stacking layers of blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # The final fully connected layer\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # Creating a layer consisting of 'num_blocks' blocks\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    ''' Basic Block for use in ResNet-18 and ResNet-34 '''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Shortcut to match the input and output dimensions\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the block\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    ''' Bottleneck Block for use in larger ResNet models (e.g., ResNet-50, 101, 152) '''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        # Three convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the block\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    ''' General ResNet model. '''\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Creating stacks of residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        # Function to create a layer of blocks\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4) # Average pooling\n",
    "        out = out.view(out.size(0), -1) # Flatten\n",
    "        out = self.linear(out) # Fully connected layer\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    ''' Function to return a ResNet-18 model. '''\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def test():\n",
    "    ''' Test function for model verification. '''\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size()) # Print the output size\n",
    "\n",
    "# Run the test function\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.parallel import DataParallel\n",
    "import time\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a series of transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    # Randomly crop the image to 32x32 pixels, adding padding of 4 pixels on each side\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    # Randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # Convert image to a PyTorch Tensor\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation for each color channel\n",
    "    # This helps in faster convergence during training\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "# The dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader for the training set\n",
    "# DataLoader provides an iterable over the given dataset, with batch processing and optional shuffling\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "# Here, shuffling is not necessary as it's used only for evaluating the model\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "    Returns:\n",
    "    float: The time taken to complete the epoch\n",
    "    \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move the data to the specified device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Clear the gradients before backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Record the end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Return the time taken for this epoch\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: Quadro RTX 8000\n",
      "Total GPU Memory: 44.48 GB\n",
      "Allocated GPU Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the first GPU\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "    # Get the properties of the first GPU\n",
    "    gpu_properties = torch.cuda.get_device_properties(0)\n",
    "\n",
    "    # Get total memory in GB\n",
    "    total_memory_gb = gpu_properties.total_memory / (1024 ** 3)\n",
    "    print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
    "\n",
    "    # Get the amount of memory currently allocated (in bytes) and convert to GB\n",
    "    allocated_memory_gb = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
    "    print(f\"Allocated GPU Memory: {allocated_memory_gb:.2f} GB\")\n",
    "else:\n",
    "    # If no GPU is available, print this message\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for batch size 32: 23.52 seconds\n",
      "Training time for batch size 128: 17.53 seconds\n",
      "Training time for batch size 512: 18.47 seconds\n",
      "Training time for batch size 2048: 19.52 seconds\n",
      "Training time for batch size 8192: 21.18 seconds\n",
      "Out of memory with batch size 32768.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "def main():\n",
    "    # Set the device to GPU if available, otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = ResNet18().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    # Starting batch size\n",
    "    batch_size = 32\n",
    "\n",
    "    # Loop to incrementally increase batch size and train the model\n",
    "    while True:\n",
    "        try:\n",
    "            # Initialize DataLoader with the current batch size\n",
    "            trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "            # Warm-up epoch: Run one epoch without timing it, for warming up the GPU\n",
    "            one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "\n",
    "            # Timed epoch: Run another epoch and measure the training time\n",
    "            start_time = time.time()\n",
    "            one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(f\"Training time for batch size {batch_size}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "            # Increase the batch size by 4 times for the next iteration\n",
    "            batch_size *= 4\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                # If a CUDA out-of-memory error occurs, report and exit the loop\n",
    "                print(f\"Out of memory with batch size {batch_size}.\")\n",
    "                break\n",
    "            else:\n",
    "                # Re-raise any other runtime errors\n",
    "                raise e\n",
    "\n",
    "# Run the main function if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The output indicates the training times for different batch sizes while training a neural network on a single GPU. As the batch size increases, there's an initial decrease in training time, showing efficiency gains from larger batches. However, beyond a certain point (batch size 2048), the training time starts to increase again, possibly due to the GPU's limitations in processing larger batches efficiently. The out-of-memory error at batch size 32768 suggests the GPU's memory capacity was exceeded, preventing further training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time for batch size 32 on 1 GPUs: 23.44 seconds\n",
      "Training time for batch size 32 on 2 GPUs: 60.17 seconds\n",
      "Training time for batch size 32 on 4 GPUs: 70.33 seconds\n",
      "Training time for batch size 128 on 1 GPUs: 17.37 seconds\n",
      "Training time for batch size 128 on 2 GPUs: 20.04 seconds\n",
      "Training time for batch size 128 on 4 GPUs: 19.99 seconds\n",
      "Training time for batch size 512 on 1 GPUs: 18.57 seconds\n",
      "Training time for batch size 512 on 2 GPUs: 11.35 seconds\n",
      "Training time for batch size 512 on 4 GPUs: 9.88 seconds\n",
      "Training time for batch size 2048 on 1 GPUs: 19.68 seconds\n",
      "Training time for batch size 2048 on 2 GPUs: 11.19 seconds\n",
      "Training time for batch size 2048 on 4 GPUs: 10.02 seconds\n",
      "Training time for batch size 8192 on 1 GPUs: 21.34 seconds\n",
      "Training time for batch size 8192 on 2 GPUs: 13.03 seconds\n",
      "Training time for batch size 8192 on 4 GPUs: 11.28 seconds\n",
      "Out of memory with batch size 32768 on 1 GPUs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import DataParallel\n",
    "import time\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch and measure the elapsed time\n",
    "    Returns:\n",
    "    float: Time taken for one epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    gpu_configs = [1, 2, 4]  # GPU configurations\n",
    "    batch_size = 32\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            for num_gpus in gpu_configs:\n",
    "                # Initialize the model\n",
    "                model = ResNet18().to(device)\n",
    "                if num_gpus > 1:\n",
    "                    model = DataParallel(model, device_ids=list(range(num_gpus)))\n",
    "\n",
    "                # Set up the optimizer\n",
    "                optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "                # Initialize DataLoader\n",
    "                trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "                # Warm-up epoch (not timed)\n",
    "                train_one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "\n",
    "                # Timed training epoch\n",
    "                training_time = train_one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "                print(f\"Training time for batch size {batch_size} on {num_gpus} GPUs: {training_time:.2f} seconds\")\n",
    "\n",
    "            # Increase batch size for next iteration\n",
    "            batch_size *= 4\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                print(f\"Out of memory with batch size {batch_size} on {num_gpus} GPUs.\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The data indicates a scaling experiment using multiple GPUs. For smaller batches (32, 128), adding more GPUs increases the training time, suggesting inefficiency, possibly due to overheads in data synchronization across GPUs. As the batch size increases (512, 2048, 8192), the training time decreases with more GPUs, showing effective utilization of additional GPUs. This scenario is indicative of strong scaling, where we fix the total problem size (batch size per GPU) and increase the resources. If weak scaling were used (increasing problem size with resources), the numbers might show less drastic speedup due to increased per-GPU computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        | Batch-size 32 per GPU |              | Batch-size 128 per GPU |              | Batch-size 512 per GPU |              |\n",
    "|--------|-----------------------|--------------|------------------------|--------------|------------------------|--------------|\n",
    "|        | Time (sec)            | Speedup      | Time (sec)             | Speedup      | Time (sec)             | Speedup      |\n",
    "| 1-GPU  | 23.44 sec             | 1x                       | 17.37 sec              | 1x    | 18.57 sec              | 1x    |\n",
    "| 2-GPU  | 60.17 sec             | 2(23.44) / 60.17 ≈ 0.7791258102x | 20.04 sec              | 2(17.37) / 20.04 ≈1.73353293413x | 11.35 sec              | 2(18.57) / 11.35 ≈ 3.27224669604x |\n",
    "| 4-GPU  | 70.33 sec             | 4(23.44) / 70.33 ≈ 1.33314375089x | 19.99 sec              | 4(17.37) / 19.99 ≈ 3.47573786893x | 9.88 sec               | 4(18.57) / 9.88 ≈ 7.51821862348x |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the speedup values, it is clear that for batch size of 32 using 2 GPUs, there's actually a decrease in performance since it takes longer to process with 2 GPUs than with just one. This suggests that for smaller batch sizes, the additional time required to coordinate between multiple GPUs may outweigh the performance gains. On the other hand, with a batch size of 32 using 4 GPUs, the speedup is approximately 2.6, which is along the lines of what we would anticipate. Moreover, when the batch sizes are increased to 128 and 512, there's a notable improvement in processing speed for configurations with 2 and 4 GPUs, relative to a single GPU setup. \n",
    "\n",
    "In this context, with Weak-Scaling, each GPU handles a constant workload despite an increase in the number of GPUs. Conversely, Strong-Scaling keeps the overall workload fixed while adding more GPUs. The experiment aligns with Weak-Scaling, as the per-GPU batch size remains unchanged while more GPUs are added. The results show non-linear efficiency in Weak-Scaling, particularly at smaller batch sizes, likely due to communication overheads and small workloads on multiple GPUs. Switching to Strong-Scaling, where the total batch size stays constant as GPUs increase, might improve efficiency for smaller batches but could result in underutilization for larger batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        | Batch-size 32 per GPU |       | Batch-size 128 per GPU |       | Batch-size 512 per GPU |       |\n",
    "|--------|-----------------------|-------|------------------------|-------|------------------------|-------|\n",
    "|        | Compute(sec)          | Comm(sec) | Compute(sec)          | Comm(sec) | Compute(sec)          | Comm(sec) |\n",
    "| 2-GPU  | 11.72                 | 48.45    | 8.685                  | 11.355   | 9.285                  | 2.065    |\n",
    "| 4-GPU  | 5.86                  | 64.47    | 4.3425                 | 15.6475  | 4.6425                 | 5.2375   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    " \n",
    " The 'Compute' time can be considered as the total training time on a single GPU divided by the number of GPUs. The 'Communication' time is the difference between the total time on multiple GPUs and the computed 'Compute' time: \n",
    "\n",
    "\n",
    "1) For 2 GPUs with batch-size 32:\n",
    "- Compute time: 23.44 / 2 = 11.72 seconds\n",
    "- Communication time: 60.17 - 11.72 = 48.45 seconds\n",
    "2) For 2 GPUs with batch-size 128:\n",
    "- Compute time: 17.37 / 2 = 8.685 seconds\n",
    "- Communication time: 20.04 − 8.685 = 11.355 seconds\n",
    "3) For 2 GPUs with batch-size 512:\n",
    "- Compute time: 18.57 / 2 = 9.285 seconds\n",
    "- Communication time: 11.35 − 9.285 = 2.065 seconds\n",
    "4) For 4 GPUs with batch-size 32:\n",
    "- Compute time: 23.44 / 4 = 5.86 seconds\n",
    "- Communication time: 70.33 − 5.86 = 64.47 seconds\n",
    "5) For 4 GPUs with batch-size 128:\n",
    "- Compute time: 17.37 / 4 = 4.3425 seconds\n",
    "- Communication time: 19.99 - 4.3425 = 15.6475 seconds\n",
    "6) For 4 GPUs with batch-size 512:\n",
    "- Compute time: 18.57 / 4 = 4.6425 seconds\n",
    "- Communication time: 9.88 - 4.6425 = 5.2375 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compute time per GPU, obtained by dividing single GPU training time by the number of GPUs, decreases as more GPUs are used. However, the time spent on communication, calculated by subtracting compute time from total multi-GPU training time, notably rises with the addition of more GPUs. This trend indicates that the communication and data synchronization overheads among multiple GPUs significantly affect the overall training efficiency as the GPU count increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Equation for Allreduce: 2(N-1)(K/N) \n",
    "  - K = # of model parameters (in this specific scenario, we are using resnet18)\n",
    "  - N = # of GPUs utilized\n",
    "\n",
    "Equation for bandwidth utilization = allreduce communication cost / communication time\n",
    "\n",
    "Allreduce cost for 2 and 4 GPUs: \n",
    "- K = # of parameters in resnet18 = 11,689,512\n",
    "    - 2-GPU allreduce cost = 2(2-1)(11689512 / 2) = 11,689,512\n",
    "    - 4-GPU allreduce cost = 2(4-1))(11689512 /4) = 17,534,268\n",
    "    Convert values to GB:\n",
    "    - 2-GPU allreduce cost = (11689512 * 4 bytes) / (2^30) = 0.044 GB\n",
    "    - 4-GPU allreduce cost = (17534268 * 4 bytes) / (2^30) = 0.065 GB\n",
    "\n",
    "2-GPU Bandwidth Utilization for different batch sizes:\n",
    "- Bandwidth Utilization: batch-size-per-GPU 32 = 0.044 / 48.45 = 0.00090815273\n",
    "- Bandwidth Utilization: batch-size-per-GPU 128 = 0.044 / 11.355 = 0.00387494495\n",
    "- Bandwidth Utilization: batch-size-per-GPU 512 = 0.044 / 2.065 = 0.02130750605\n",
    "\n",
    "4-GPU Bandwidth Utilization for different batch sizes:\n",
    "- Bandwidth Utilization: batch-size-per-GPU 32 = 0.065 / 64.47 = 0.00100822087\n",
    "- Bandwidth Utilization: batch-size-per-GPU 128 = 0.065 / 15.6475 = 0.00415401821\n",
    "- Bandwidth Utilization: batch-size-per-GPU 512 = 0.065 / 5.2375 = 0.01241050119\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|        | Batch-size-per-GPU 32                  | Batch-size-per-GPU 128                 | Batch-size-per-GPU 512                 |\n",
    "|--------|----------------------------------------|----------------------------------------|----------------------------------------|\n",
    "|        | Bandwidth Utilization (GB/s)           | Bandwidth Utilization (GB/s)           | Bandwidth Utilization (GB/s)           |\n",
    "| 2-GPU  | 0.00090815273 |0.00387494495  |0.02130750605|\n",
    "| 4-GPU  | 0.00100822087|0.00415401821   |0.01241050119 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the values show that as the batch size per GPU increases, the bandwidth utilization improves. For 2 GPUs, smaller batch sizes lead to lower efficiency due to increased communication overhead. However, with 4 GPUs, the efficiency is generally better, suggesting that larger batch sizes are more effective in utilizing communication bandwidth efficiently in a parallel training scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
