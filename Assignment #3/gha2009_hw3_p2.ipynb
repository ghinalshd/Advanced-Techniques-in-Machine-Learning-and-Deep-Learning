{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2 - Neural Network Training and Backpropagation"
      ],
      "metadata": {
        "id": "NcDhlfqyBd6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "%matplotlib inline\n",
        "#Load data and create X and y variables\n",
        "data = loadmat('/content/sample_data/ex3data1.mat')\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zP1v2LSFQe-",
        "outputId": "44be1ddf-7b5f-4814-df04-94dda5f3b9f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
              " '__version__': '1.0',\n",
              " '__globals__': [],\n",
              " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " 'y': array([[10],\n",
              "        [10],\n",
              "        [10],\n",
              "        ...,\n",
              "        [ 9],\n",
              "        [ 9],\n",
              "        [ 9]], dtype=uint8)}"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUpeqh8wGBe3",
        "outputId": "4ecfa136-8abd-4e4e-b417-ffff4d3abebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 400), (5000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One-hot encode y labels to transform 'y' variables into 0 and 1 values\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "print(y_onehot.shape)\n",
        "y[0], y_onehot[0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msjaH0EcGJf-",
        "outputId": "3e12e143-178b-4bb8-eb01-7628b93f2f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([10], dtype=uint8), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1"
      ],
      "metadata": {
        "id": "-id00ye6CNLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid Function\n",
        "def sigmoid(z):\n",
        "  #Scaling the input 'z' for the sigmoid function makes the curve steeper\n",
        "  return 1 / (1 + np.exp(-2 * z))"
      ],
      "metadata": {
        "id": "vTNdohMM9x99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2"
      ],
      "metadata": {
        "id": "e1D_yfneCWqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters:\n",
        "\n",
        "    - X → Input features\n",
        "    - theta1, theta2, theta3 → Weights for each layer\n",
        "Returns:\n",
        "\n",
        "    - Activations and weighted sums for each layer\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8GcJulYYONY0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaITPmRaf-BK"
      },
      "outputs": [],
      "source": [
        "#Forward_propagate() Function\n",
        "def forward_propagate(X, theta1, theta2, theta3):\n",
        "  m = X.shape[0]\n",
        "  #Layer 1: Input Layer\n",
        "  a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
        "  #Layer 2: First Hidden Layer\n",
        "  z2 = a1 * theta1.T #Changed from element-wise multiplication to matrix multiplication\n",
        "  a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1) #Using the scaled sigmoid function\n",
        "  #Layer 3: Second Hidden Layer\n",
        "  z3 = a2 * theta2.T\n",
        "  a3 = np.insert(sigmoid(z3), 0, values=np.ones(m), axis=1)\n",
        "  #Layer 4: Output Layer\n",
        "  z4 = a3 * theta3.T\n",
        "  h = sigmoid(z4)\n",
        "  return a1, z2, a2, z3, a3, z4, h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3"
      ],
      "metadata": {
        "id": "_7R7c6Mv91wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cost() Function Without Regularization\n",
        "def cost_no_regularization(params, input_size, hidden_size1, hidden_size2, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # Reshape params to get weights for each layer\n",
        "    limit0 = hidden_size1 * (input_size + 1)\n",
        "    limit1 = limit0 + hidden_size2 * (hidden_size1 + 1)\n",
        "    theta1 = np.reshape(params[:limit0], (hidden_size1, input_size + 1))\n",
        "    theta2 = np.reshape(params[limit0:limit1], (hidden_size2, hidden_size1 + 1))\n",
        "    theta3 = np.reshape(params[limit1:], (num_labels, hidden_size2 + 1))\n",
        "\n",
        "    # Forward propagate\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # Compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    return J / m"
      ],
      "metadata": {
        "id": "FGpDkL9--AIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cost() Function With Regularization\n",
        "def cost_with_regularization(params, input_size, hidden_size1, hidden_size2, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # Reshape params to get weights for each layer\n",
        "    limit0 = hidden_size1 * (input_size + 1)\n",
        "    limit1 = limit0 + hidden_size2 * (hidden_size1 + 1)\n",
        "    theta1 = np.reshape(params[:limit0], (hidden_size1, input_size + 1))\n",
        "    theta2 = np.reshape(params[limit0:limit1], (hidden_size2, hidden_size1 + 1))\n",
        "    theta3 = np.reshape(params[limit1:], (num_labels, hidden_size2 + 1))\n",
        "\n",
        "    # Forward propagate\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # Compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    # Add regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta2[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta3[:,1:], 2)))\n",
        "\n",
        "    return J"
      ],
      "metadata": {
        "id": "1qvvtJX391wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initial setup\n",
        "input_size = 400\n",
        "hidden_size1 = 20\n",
        "hidden_size2 = 20\n",
        "num_labels = 10\n",
        "learning_rate = 1\n",
        "\n",
        "# randomly initialize a parameter array of the size of the full network's parameters\n",
        "params = (np.random.random(size=hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1) + num_labels * (hidden_size2 + 1)) - 0.5) * 0.25\n",
        "\n",
        "m = X.shape[0]\n",
        "X = np.matrix(X)\n",
        "y = np.matrix(y)\n",
        "\n",
        "theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1)], (hidden_size2, (hidden_size1 + 1))))\n",
        "theta3 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1):], (num_labels, (hidden_size2 + 1))))\n",
        "theta1.shape, theta2.shape, theta3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFBJlcz4oMOK",
        "outputId": "2eea80d1-222b-4528-9341-f0cf63793012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20, 401), (20, 21), (10, 21))"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "a1.shape, z2.shape, a2.shape, z3.shape, a3.shape, z4.shape, h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux-WCzcjocl7",
        "outputId": "b5cb5f2b-ce6d-4681-d621-afa967ee7e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((5000, 401),\n",
              " (5000, 20),\n",
              " (5000, 21),\n",
              " (5000, 20),\n",
              " (5000, 21),\n",
              " (5000, 10),\n",
              " (5000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost_no_regularization(params, input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzNLPZSgoIEw",
        "outputId": "398c2706-838e-4a8c-cc98-c9a5e5936ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.902314128126856"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cost_with_regularization(params, input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QolKSNazo4TM",
        "outputId": "8ad5eaf9-51f0-4282-996a-ec3d886dd859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.906787101983319"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4"
      ],
      "metadata": {
        "id": "nrRm_0lM914k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid_gradient() Function\n",
        "def sigmoid_gradient(z):\n",
        "    return 2 * np.multiply(sigmoid(z), (1 - sigmoid(z)))"
      ],
      "metadata": {
        "id": "WGKHQ89q914l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5"
      ],
      "metadata": {
        "id": "UFAnt75y92BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Backprop() Without Regularization\n",
        "def backprop(params, input_size, hidden_size1, hidden_size2, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1)], (hidden_size2, (hidden_size1 + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1):], (num_labels, (hidden_size2 + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(theta1.shape)\n",
        "    delta2 = np.zeros(theta2.shape)\n",
        "    delta3 = np.zeros(theta3.shape)\n",
        "\n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)))\n",
        "\n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]\n",
        "        z2t = z2[t,:]\n",
        "        a2t = a2[t,:]\n",
        "        z3t = z3[t,:]\n",
        "        a3t = a3[t,:]\n",
        "        ht = h[t,:]\n",
        "        yt = y[t,:]\n",
        "\n",
        "        d4t = ht - yt\n",
        "        z3t = np.insert(z3t, 0, values=np.ones(1))\n",
        "        d3t = np.multiply((theta3.T * d4t.T).T, sigmoid_gradient(z3t))\n",
        "\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))\n",
        "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, sigmoid_gradient(z2t))\n",
        "\n",
        "        delta1 += (d2t[:,1:]).T * a1t\n",
        "        delta2 += (d3t[:,1:]).T * a2t\n",
        "        delta3 += d4t.T * a3t\n",
        "\n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    delta3 = delta3 / m\n",
        "\n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
        "\n",
        "    return J, grad"
      ],
      "metadata": {
        "id": "kNhNbo8Y-KLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J, grad = backprop(params, input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CMdFJ9EpDnp",
        "outputId": "ab13388f-89ef-4b44-da15-b82365a4ad82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.906695011130548, (8650,))"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Backprop() With Regularization\n",
        "def backprop_with_regularization(params, input_size, hidden_size1, hidden_size2, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    #Reshape the parameter array into parameter matrices for each layer\n",
        "    theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1)], (hidden_size2, (hidden_size1 + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1):], (num_labels, (hidden_size2 + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    #Initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(theta1.shape)\n",
        "    delta2 = np.zeros(theta2.shape)\n",
        "    delta3 = np.zeros(theta3.shape)\n",
        "\n",
        "    #Compute the cost with regularization\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)) + np.sum(np.power(theta3[:,1:], 2)))\n",
        "\n",
        "    #Perform backpropagation with regularization\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]\n",
        "        z2t = z2[t,:]\n",
        "        a2t = a2[t,:]\n",
        "        z3t = z3[t,:]\n",
        "        a3t = a3[t,:]\n",
        "        ht = h[t,:]\n",
        "        yt = y[t,:]\n",
        "\n",
        "        d4t = ht - yt\n",
        "        z3t = np.insert(z3t, 0, values=np.ones(1))\n",
        "        d3t = np.multiply((theta3.T * d4t.T).T, sigmoid_gradient(z3t))\n",
        "\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))\n",
        "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, sigmoid_gradient(z2t))\n",
        "\n",
        "        delta1 += (d2t[:,1:]).T * a1t\n",
        "        delta2 += (d3t[:,1:]).T * a2t\n",
        "        delta3 += d4t.T * a3t\n",
        "\n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    delta3 = delta3 / m\n",
        "\n",
        "    delta1[:,1:] += (theta1[:,1:] * learning_rate) / m\n",
        "    delta2[:,1:] += (theta2[:,1:] * learning_rate) / m\n",
        "    delta3[:,1:] += (theta3[:,1:] * learning_rate) / m\n",
        "\n",
        "    #Unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
        "\n",
        "    return J, grad"
      ],
      "metadata": {
        "id": "_LOlywen92BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J, grad = backprop(params, input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkUPCWODpE2D",
        "outputId": "e1c1fcae-52a8-4d37-e11c-8fe36d2c9b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.906695011130548, (8650,))"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6"
      ],
      "metadata": {
        "id": "DpmTkUOL92Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# Hyperparameters and initial setup\n",
        "input_size = 400   #Each input feature vector is of size 400\n",
        "hidden_size1 = 20  #Number of units in the first hidden layer\n",
        "hidden_size2 = 20  #Number of units in the second hidden layer\n",
        "num_labels = 10    #Number of output classes\n",
        "learning_rate = 1  #As mentioned in the question to keep unchanged\n",
        "\n",
        "# Randomly initialize weights\n",
        "params = (np.random.random(size=hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1) + num_labels * (hidden_size2 + 1)) - 0.5) * 0.25\n",
        "\n",
        "# Minimize the objective function\n",
        "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate),\n",
        "                method='TNC', jac=True, options={'maxiter': 250})\n",
        "\n",
        "# Extract trained parameters\n",
        "X = np.matrix(X)\n",
        "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(fmin.x[hidden_size1 * (input_size + 1):hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1)], (hidden_size2, (hidden_size1 + 1))))\n",
        "theta3 = np.matrix(np.reshape(fmin.x[hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1):], (num_labels, (hidden_size2 + 1))))"
      ],
      "metadata": {
        "id": "oKP1F-jP92Hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31146ef7-a181-47b1-a8da-eb5c961729d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-126-c3104bb7c335>:14: OptimizeWarning: Unknown solver options: maxiter\n",
            "  fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7"
      ],
      "metadata": {
        "id": "gLPj7Jou92OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXjyFMypt0L-",
        "outputId": "7868294a-d2d2-4046-a30a-324cd9db1a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10],\n",
              "       [10],\n",
              "       [10],\n",
              "       ...,\n",
              "       [ 9],\n",
              "       [ 9],\n",
              "       [ 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
        "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "print('accuracy = {0}%'.format(accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AseFrG8rKw7",
        "outputId": "d01b217f-bec0-42a1-8ea3-ea0870c33b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy = 99.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8"
      ],
      "metadata": {
        "id": "QdXYjW4l92VA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "- The 3-layer neural network model that we implemented showed a slight improvement in accuracy, achieving 99.68%, compared to the 2-layer neural network from the original notebook which had an accuracy of 99.22%.\n",
        "- This suggests that the additional hidden layer in our model may have captured more complex features or patterns in the data, leading to a marginally better performance.\n",
        "- However, it is essential to note that while the improvement is positive, it's relatively minor. In reality, the decision to add more layers should be balanced with the potential risk of overfitting.\n",
        "- In this case, the addition of an extra layer did provide a performance boost, albeit a small one."
      ],
      "metadata": {
        "id": "45i7WFxc-Wyu"
      }
    }
  ]
}