{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 3 - Attention in Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kineJbrfcg7"
      },
      "source": [
        "**Answer:**\n",
        "From each of the encoderâ€™s input vectors (e.g., the embedding of each word), how many vectors are\n",
        "derived in a self-attention block in a Transformer? What are these vectors called? (2)\n",
        "\n",
        "There are three vectors derived in a self-attention block in a Transformer: Query (Q), Key (K), and Value (V). The vectors are calculated by multiplying the input vector with three different weight matrices, which are learned during the training process. In other words, for each word in the input sequence, we get one set of the Q, K, and V vectors. These vectors are meant to create a system where every word in the input sequence is compared to every other word, capturing the contextual links between the words. Specifically:\n",
        "\n",
        "- All of the words in the input are represented as key vectors, which are then compared to query vectors.\n",
        "- The word representations that will be utilized to build the self-attention layer's output are contained in the value vector.\n",
        "- Value vectors are weighted by these attention scores to produce the output of the self-attention layer for each word, whereas Query and Key vectors are used to calculate attention scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX8Ns_Imf-BI"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "When encoding a specific piece of the input sequence, attention scores are computed to decide how much attention and focus should be provided to the other sections of the sequence. To do this, a scaled dot-product attention computation is carried out.\n",
        "\n",
        "1) The key vectors of every word in the sequence are dot-multiplied with the query vector of that specific word. These dot products provide scores that reflect how much each word should attend to every other word.\n",
        "\n",
        "2) To assist in stabilizing the gradients during training, these scores are subsequently divided by the square root of the dimensionality of the key vectors.\n",
        "\n",
        "3) These scores are then scaled, and a softmax function is performed along each row, transforming them into probabilities. By ensuring that the attention scores for every word in the sequence add up to 1, the softmax step creates a weighted representation of the entire sequence based on each word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMHZDOECjD3"
      },
      "source": [
        "## 3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRo9fm0tf-ga"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "- Given an eight-headed multi-headed attention mechanism with input and output vectors of size 512 for each head, a total of 24 weight matrices are needed (8 matrices for Q, K, and V).\n",
        "- Different parts of the contextual information are captured by each head in the multi-head attention mechanism as it develops a distinct representation (or \"subspace\") of the input sequence.\n",
        "- Each of these 24 matrices would be 512x512 in size for an input of size 4 word embeddings (assuming each embedding is of size 512).\n",
        "- These matrices transfer each 512-dimensional input vector (word embedding) to a 512-dimensional Q, K, or V vector, which explains why the 512x512 dimension exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri59wswo8wSo"
      },
      "source": [
        "## 3.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF8uLB3J8wSp"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "After passing each word through its self-attention mechanism in each head, we arrive at eight distinct output vectors (assuming eight heads). For every word, these vectors are concatenated to create a single, lengthy vector. The ensuing feed-forward network, however, expects input of the original size because this concatenated vector is larger than the original embedding size. In order to fix this, the concatenated vector is linearly transformed back down to the original embedding size using a second learned weight matrix. This transformation is essential because it pulls together data from each head, giving the model the ability to use various learned sequence elements as it moves through the remaining Transformer layers."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
