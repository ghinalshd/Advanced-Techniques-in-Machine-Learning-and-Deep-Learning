{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 3 - Weakly and Semi-Supervised Learning for Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kineJbrfcg7"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "- Weakly Supervised Pretraining:\n",
        "  - This approach involves training models with data that have been labeled in a less precise manner. The labels used are not crafted through meticulous human annotation but are instead derived from an alternative, less precise source. The hashtags from social media used in the referenced article serve as an example of such weak supervision. These hashtags give a general context to the image but do not provide detailed, descriptive labels.\n",
        "\n",
        "- Semi-Supervised Pretraining:\n",
        "  - This method leverages a combination of both annotated and non-annotated data. It starts with a relatively small set of accurately labeled data and expands its learning with a more extensive set of unlabeled data. The foundational idea here is that the unlabeled data shares a similar pattern or distribution with the labeled data, which should, in theory, aid the model in improving its ability to generalize.\n",
        "\n",
        "- Utilizing a Unified Dataset (1 Billion Image Set):\n",
        "  - In the context of weakly supervised pretraining, this process entails utilizing associated hashtags as imprecise labels, which, despite their lack of direct correlation to the content of the images, provide enough context for the model to begin forming generalizations.\n",
        "  - On the semi-supervised pretraining side, the process is more intricate. It starts with a specific subset of the data that includes 1.2 million images across 1000 classes, which have been accurately labeled for the purpose of enhancing both accuracy and generalization. This smaller, labeled dataset is used to train a 'teacher' model, which then annotates the larger body of unlabeled data by assigning labels with the highest confidence scores. These pseudo-labeled images are then sorted by their predicted confidence scores, creating a prioritized basis for the 'student' model to continue its training and refining process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-swV-4F8pso"
      },
      "source": [
        "### 3.2(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX8Ns_Imf-BI"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "In their study, Mahajan and colleagues explored the resilience of models trained on noisy social media hashtag labels. They hypothesized that with a sufficiently large dataset, the models would tolerate label inaccuracies. To test this, they experimented with various dataset sizes and evaluated the models against the ImageNet benchmark. They conducted tests using four different combinations: training with 1.5k hashtags on 940 million images; 8.5k hashtags on 1 billion images; 17k hashtags on 1 billion images; and lastly, 17k hashtags on 3.5 billion images. These combinations were further tested across three different class sizes – 1k, 5k, and 9k – to compare against ImageNet results.\n",
        "\n",
        "The outcomes were in line with expectations. The paper reveals that the hashtag-based models surpassed ImageNet accuracy when targeting 5,000 and 9,000 classes. Specifically, the model trained on the smallest dataset outperformed ImageNet by 1.9 percentage points. As anticipated, a higher number of classes led to a drop in accuracy due to the increased likelihood of errors and the challenges of more granular classification. On the other hand, the findings for the 1,000-class scenario had the highest accuracy rate of 84.2%, which is a result of model training on the smallest dataset. This accuracy equaled that of the largest model and significantly exceeded the ImageNet model's 79.6% accuracy, underscoring the robustness of models trained on vast amounts of noisy data.\n",
        "\n",
        "The researchers additionally introduced varying levels of noise (10%, 25%, 50%) to the hashtags in the training data to observe the impact on accuracy for different class counts in the target dataset (ImageNet). The models showed a remarkable tolerance to the noise, with minimal performance degradation.\n",
        "\n",
        "Overall, the study demonstrated that models using hashtags can effectively withstand noisy labels, particularly when trained with large datasets and numerous hashtags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4O1Y1gG8sVR"
      },
      "source": [
        "### 3.2(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLttsWNl8sVS"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "To address the skewness towards prevalent classes and improve model performance on rarer ones, Mahajan and co-researchers employed a resampling technique during the pretraining stage. This method deliberately decreases the representation of overrepresented hashtags while amplifying the presence of the rarer ones, fostering a more equitable dataset. Such a balanced dataset equips the model to extract more generalized and unbiased features, as opposed to learning predominantly from the overrepresented classes.\n",
        "\n",
        "Additionally, this approach enhances the model's ability to adapt to different tasks and datasets, where class distributions may vary significantly from the pretraining data. The resampling technique not only aids in generalizing across different domains but also mitigates the effect of label noise by promoting a richer diversity of examples.\n",
        "\n",
        "In conclusion, resampling stands as a critical step for ensuring that the model can generalize well to new, unseen data, thereby enhancing its capability for transfer learning and avoids overfitting. It ensures that models trained on such data are capable of delivering unbiased predictions across a range of subjects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMHZDOECjD3"
      },
      "source": [
        "## 3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu0xnp_a8t5H"
      },
      "source": [
        "### 3.3(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRo9fm0tf-ga"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The paper introduces a dual-model structure composed of 'teacher' and 'student' networks, which utilize a method known as distillation. The 'teacher' is an advanced network trained on a substantial amount of labeled data, which it then uses to generate pseudo-labels for a vast collection of unlabeled data. Subsequently, the 'student' model is trained on a mix of original labeled data and the new pseudo-labeled data derived from the teacher. The student model benefits from the teacher by aiming to replicate the teacher's predictions on unlabeled data, thereby gaining insights into both the features and the teacher's confidence levels about the data.\n",
        "\n",
        "The student model's training involves reducing the discrepancy between its own output and that of the teacher's on the unlabeled dataset, which in turn allows it to capture more subtle feature representations.\n",
        "\n",
        "This approach is deemed a distillation process because it involves the teacher model—often larger and more complex—imparting knowledge to the student model, which is typically smaller and less complex. The student benefits from an enriched training experience by not only having access to a greater quantity of labeled data but also by learning from the probability outputs of the teacher model, which can lead to an improved understanding of the data features and nuances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri59wswo8wSo"
      },
      "source": [
        "### 3.3(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF8uLB3J8wSp"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "In the model, 'K' denotes the quantity of images per category utilized for training the student models, drawn from a broader pool of unlabeled data. These images are chosen based on the teacher model's prediction confidence and the selection is tailored to the size of the dataset. For instance, K is set to 16,000 for datasets containing 100 million images, 8,000 for 50 million, and 4,000 for 25 million.\n",
        "\n",
        "'P' refers to the count of partitions that each training sample can be part of. When P is greater than 1, it allows for any given training example to appear in multiple sets, leading to a more evenly distributed class representation throughout the training process. This approach specifically addresses the issue of certain classes being underrepresented, reducing the likelihood that a few classes might dominate and bias the learning process. By doing so, it helps in avoiding overfitting and improves the performance of the student model. Choosing higher values of P enhances the diversity within the training set, further contributing to the robustness of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0afIfh_8wXR"
      },
      "source": [
        "### 3.3(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX6hArot8wXR"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The formation of a labeled dataset from unlabeled imagery involves processing these images through the teacher model, which then categorizes them. For each image, the model retrieves the top 'K' class predictions along with their respective confidence levels. When the confidence for the top predicted classes exceeds a certain threshold, the image is tagged with that particular class. Conversely, if the confidence for the top prediction falls below the threshold, the image may be assigned to several classes, up to a maximum of 'P'. Hence, it is feasible for an image within this newly formed dataset to be associated with multiple classes, capped at 'P'. This methodology acknowledges the reality that many images naturally embody more than one class—such thereby enabling the student model to recognize and predict multiple class affiliations for a single image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC1HEHCe8wbg"
      },
      "source": [
        "### 3.3(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YNAJLoU8wbg"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "The student model's accuracy initially rises with an increase in 'K', reaching a peak before it starts to decline. This trend suggests that, initially, as 'K' grows, the student model benefits by learning richer patterns within the data. Eventually, an excessively high 'K' leads the student model to essentially memorize the data, including any errors or 'noise' present in the teacher model's predictions, which in turn degrades its own predictive performance. The ideal 'K' value is contingent upon the teacher model's precision and the quality of the unlabeled dataset. When the teacher model is highly accurate and the data is of good quality, enlarging 'K' allows the student to discern more intricate class relationships. On the other hand, if the teacher's model is inaccurate is  or the data is fraught with noise, a large 'K' will cause the student to adopt the teacher's inaccuracies. Therefore, determining the optimal 'K' is necessary throughout the study."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
